{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp041\n",
    "word2vecのcand検討 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "import gc\n",
    "import random\n",
    "import pickle\n",
    "import pathlib\n",
    "import subprocess\n",
    "from dataclasses import dataclass\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv\n",
    "sys.path.append(os.getenv('UTILS_PATH'))\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "import inspect\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import cudf\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import line_notify\n",
    "import my_logger\n",
    "from noglobal import noglobal\n",
    "import optuna\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.similarities.annoy import AnnoyIndexer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Cfg:\n",
    "    loglevel = \"INFO\"\n",
    "    exp_name = \"exp030\"\n",
    "    seed = 42\n",
    "    k = 20\n",
    "    cand_n = 10\n",
    "    type2id = {\"clicks\":0, \"carts\":1, \"orders\":2}\n",
    "    id2type = {0:\"clicks\", 1:\"carts\", 2:\"orders\"}\n",
    "    train_weeks = [\"week3\"]\n",
    "    valid_week = \"week4\"\n",
    "    valid_session_n = 100_000\n",
    "    input_dir = os.getenv('INPUT_DIR')\n",
    "    output_dir = os.getenv('OUTPUT_DIR')\n",
    "    prep_dir = os.getenv(\"PREP_DIR\")\n",
    "\n",
    "cfg = Cfg()\n",
    "os.makedirs(os.path.join(cfg.output_dir, cfg.exp_name), exist_ok=True)\n",
    "os.makedirs(os.path.join(cfg.output_dir, cfg.exp_name, \"cache\"), exist_ok=True)\n",
    "random.seed(cfg.seed)\n",
    "\n",
    "logger = my_logger.init_logger(cfg.exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@noglobal(excepts=[\"cfg\", \"logger\"])\n",
    "def evaluate(clicks_labels, carts_labels, orders_labels, \n",
    "             clicks_preds, carts_preds, orders_preds, k=20):\n",
    "\n",
    "    num_clicks = 0\n",
    "    num_carts = 0\n",
    "    num_orders = 0\n",
    "    hit_clicks = 0\n",
    "    hit_carts = 0\n",
    "    hit_orders = 0\n",
    "\n",
    "    for i in range(len(clicks_labels)):\n",
    "        clicks_label = clicks_labels[i]\n",
    "        carts_label = carts_labels[i]\n",
    "        orders_label = orders_labels[i]\n",
    "        clicks_pred = clicks_preds[i]\n",
    "        carts_pred = carts_preds[i]\n",
    "        orders_pred = orders_preds[i]\n",
    "\n",
    "        if type(clicks_pred) == list:\n",
    "            clicks_pred = clicks_pred[:k]\n",
    "        else:\n",
    "            clicks_pred = []\n",
    "        if type(carts_pred) == list:\n",
    "            carts_pred = carts_pred[:k]\n",
    "        else:\n",
    "            carts_pred = []    \n",
    "        if type(orders_pred) == list:\n",
    "            orders_pred = orders_pred[:k]\n",
    "        else:\n",
    "            orders_pred = []\n",
    "\n",
    "        if not np.isnan(clicks_label):\n",
    "            num_clicks += 1\n",
    "            hit_clicks += int(clicks_label in clicks_pred)\n",
    "\n",
    "        if type(carts_label) == list:\n",
    "            num_carts += min(len(carts_label), k)\n",
    "            hit_carts += len(set(carts_pred) & set(carts_label))\n",
    "            \n",
    "        if type(orders_label) == list:\n",
    "            num_orders += min(len(orders_label), k)\n",
    "            hit_orders += len(set(orders_pred) & set(orders_label))\n",
    "\n",
    "\n",
    "    recall_clicks = hit_clicks / num_clicks\n",
    "    recall_carts = hit_carts / num_carts\n",
    "    recall_orders = hit_orders / num_orders\n",
    "    w_recall_clicks = recall_clicks * 0.10\n",
    "    w_recall_carts = recall_carts * 0.30\n",
    "    w_recall_orders = recall_orders * 0.60\n",
    "    score = w_recall_clicks + w_recall_carts + w_recall_orders\n",
    "\n",
    "    results = {}\n",
    "    results[\"num_clicks\"] = num_clicks\n",
    "    results[\"hit_clicks\"] = hit_clicks\n",
    "    results[\"num_carts\"] = num_carts\n",
    "    results[\"hit_carts\"] = hit_carts\n",
    "    results[\"num_orders\"] = num_orders\n",
    "    results[\"hit_orders\"] = hit_orders\n",
    "    results[\"recall_clicks\"] = format(recall_clicks, \".3f\")\n",
    "    results[\"recall_carts\"] = format(recall_carts, \".3f\")\n",
    "    results[\"recall_orders\"] = format(recall_orders, \".3f\")\n",
    "    results[\"w_recall_clicks\"] = format(w_recall_clicks, \".3f\")\n",
    "    results[\"w_recall_carts\"] = format(w_recall_carts, \".3f\")\n",
    "    results[\"w_recall_orders\"] = format(w_recall_orders, \".3f\")\n",
    "    results[\"score\"] = format(score, \".3f\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Candidate:\n",
    "    @noglobal(excepts=[\"cfg\", \"logger\"])\n",
    "    def __init__(self, pdf, labels=None):\n",
    "        self.df = cudf.from_pandas(pdf)\n",
    "        self.target_sessions = pdf[\"session\"].unique().tolist()\n",
    "        self.results = pd.DataFrame(columns=[\"name\", \"num_clicks\", \"hit_clicks\", \"num_carts\", \"hit_carts\", \"num_orders\", \"hit_orders\", \n",
    "                                             \"recall_clicks\", \"recall_carts\", \"recall_orders\", \"w_recall_clicks\", \"w_recall_carts\", \"w_recall_orders\", \"score\"])\n",
    "        self.output = pd.DataFrame(columns=[\"session\", \"aid\", \"strategy\", \"rank\"], dtype=int)\n",
    "        self.labels = labels\n",
    "    \n",
    "    @noglobal(excepts=[\"cfg\", \"logger\"])\n",
    "    def _entry(self, new_candidate_df, name, k):\n",
    "        logger.info(f\"[add_candidate] {name} : start\")\n",
    "        new_candidate_df[\"strategy\"] = name\n",
    "        new_candidate_df[f\"rank\"] = new_candidate_df.groupby(\"session\")[\"session\"].cumcount() + 1\n",
    "        new_candidate_df = new_candidate_df.to_pandas()\n",
    "        self.output = pd.concat([self.output, new_candidate_df[[\"session\", \"aid\", \"strategy\", \"rank\"]]])\n",
    "\n",
    "        if self.labels is not None:\n",
    "            self._eval(new_candidate_df[[\"session\", \"aid\"]], name, k)\n",
    "    \n",
    "    @noglobal(excepts=[\"cfg\", \"logger\"])\n",
    "    def _eval(self, new_candidate_df, name, k):\n",
    "        new_candidate_df = new_candidate_df.groupby(\"session\")[\"aid\"].apply(list).reset_index()\n",
    "        eval_df = pd.DataFrame(self.target_sessions, columns=[\"session\"])\n",
    "        eval_df = eval_df.merge(new_candidate_df, on=[\"session\"], how=\"left\")\n",
    "        assert eval_df[\"session\"].tolist() == self.labels[\"session\"].tolist()\n",
    "        eval_result = evaluate(self.labels[\"clicks_labels\"].tolist(), self.labels[\"carts_labels\"].tolist(), self.labels[\"orders_labels\"].tolist(),\n",
    "                               eval_df[\"aid\"].tolist(), eval_df[\"aid\"].tolist(), eval_df[\"aid\"].tolist(), k)\n",
    "        \n",
    "        logger.info(str(eval_result))\n",
    "        self.results = pd.concat([self.results, pd.DataFrame([[name] + list(eval_result.values())], columns=[\"name\"] + list(eval_result.keys()))])\n",
    "\n",
    "    @noglobal(excepts=[\"cfg\", \"logger\"])\n",
    "    def add(self, strategy, name, usetypes=['clicks', 'carts', 'orders'], trans_map=None, k=20):\n",
    "        logger.info(f\"[make_candidate] {name} : start\")\n",
    "        candidate_df = self.df[self.df[\"type\"].isin(usetypes)].copy()\n",
    "        \n",
    "        if strategy == \"session_frequent\":\n",
    "            candidate_df = self._session_frequent(candidate_df, k)\n",
    "        elif strategy == \"session_latest\":\n",
    "            candidate_df = self._session_latest(candidate_df, k)\n",
    "        elif strategy == \"total_frequent\":\n",
    "            candidate_df = self._total_frequent(candidate_df, k)\n",
    "        \n",
    "        if trans_map is not None:\n",
    "            candidate_df[\"aid\"] = candidate_df[\"aid\"].map(trans_map)\n",
    "            candidate_df = candidate_df.dropna(subset=[\"aid\"])\n",
    "            \n",
    "        self._entry(candidate_df, name, k)\n",
    "\n",
    "    @noglobal(excepts=[\"cfg\", \"logger\"])\n",
    "    def _session_frequent(self, candidate_df, k):\n",
    "        candidate_df = candidate_df.groupby([\"session\", \"aid\"])[\"ts\"].count().reset_index()\n",
    "        candidate_df.columns = [\"session\", \"aid\", \"aid_count\"]\n",
    "        candidate_df = candidate_df.sort_values([\"session\", \"aid_count\", \"aid\"], ascending=(True, False, True))\n",
    "        candidate_df = candidate_df[candidate_df.groupby(\"session\")[\"aid_count\"].cumcount() < k].copy()\n",
    "        candidate_df = candidate_df[[\"session\", \"aid\"]].copy()\n",
    "        return candidate_df\n",
    "\n",
    "    @noglobal(excepts=[\"cfg\", \"logger\"])\n",
    "    def _session_latest(self, candidate_df, k):\n",
    "        candidate_df = candidate_df.sort_values([\"session\", \"ts\"], ascending=(True, False)).drop_duplicates(subset=[\"session\", \"aid\"])\n",
    "        candidate_df = candidate_df.sort_values([\"session\", \"ts\"], ascending=(True, False))[candidate_df.groupby(\"session\")[\"ts\"].cumcount() < k].copy()\n",
    "        candidate_df = candidate_df[[\"session\", \"aid\"]].copy()\n",
    "        return candidate_df\n",
    "\n",
    "    @noglobal(excepts=[\"cfg\", \"logger\"])\n",
    "    def _total_frequent(self, candidate_df, k):\n",
    "        candidate_df = candidate_df.groupby(\"aid\")[\"ts\"].count().reset_index()\n",
    "        candidate_df.columns = [\"aid\", \"aid_count\"]\n",
    "        topk_freq_aids = candidate_df.sort_values(\"aid_count\", ascending=False)[\"aid\"].to_arrow().to_pylist()[:k]\n",
    "        sessions = []\n",
    "        aids = []\n",
    "        for session, aid in itertools.product(self.target_sessions, topk_freq_aids):\n",
    "            sessions.append(session)\n",
    "            aids.append(aid)\n",
    "        candidate_df = cudf.DataFrame({\"session\": sessions, \"aid\": aids})\n",
    "        return candidate_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@noglobal(excepts=[\"cfg\", \"logger\"])\n",
    "def make_cand_df(sessions, labels=None, trans_maps=None):\n",
    "\n",
    "    if labels is not None:\n",
    "         cand = Candidate(sessions, labels)\n",
    "    else:\n",
    "        cand = Candidate(sessions)\n",
    "\n",
    "    cand.add(strategy=\"session_latest\", name=\"session_latest\", k=cfg.cand_n)\n",
    "    cand.add(strategy=\"session_latest\", trans_map=trans_maps[\"word2vec_sim_pair\"], name=\"word2vec_sim_pair\", k=cfg.cand_n)\n",
    "    if labels is not None:\n",
    "        return cand.output, cand.results\n",
    "    else:\n",
    "        return cand.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@noglobal(excepts=[\"cfg\", \"logger\"])\n",
    "def get_trans_maps():\n",
    "    \"\"\"\n",
    "    aidを別のaidに置き換えるmapping辞書を取得する\n",
    "    \"\"\"\n",
    "    trans_maps = {}\n",
    "\n",
    "    pair_df_org = pd.read_pickle(cfg.prep_dir + \"co_visitation_matrix_v5.pkl\")\n",
    "    pair_df_org = pair_df_org[pair_df_org[\"cnt\"] > 3].copy()\n",
    "    pair_df_org = pair_df_org.sort_values([\"aid_x\", \"cnt\"], ascending=(True, False))\n",
    "    for i in range(10):\n",
    "        pair_df = pair_df_org.groupby(\"aid_x\").nth(i).reset_index()[[\"aid_x\", \"aid_y\"]]\n",
    "        pair_dict = {k: v for k, v in zip(pair_df[\"aid_x\"].tolist(), pair_df[\"aid_y\"].tolist())}\n",
    "        trans_maps[f\"pair{str(i)}\"] = pair_dict\n",
    "\n",
    "    pair_df = pd.read_pickle(cfg.prep_dir + \"co_visitation_matrix_clicks.pkl\")\n",
    "    pair_df = pair_df[pair_df[\"cnt\"] > 3].copy()\n",
    "    pair_df = pair_df.sort_values([\"aid_x\", \"cnt\"], ascending=(True, False))\n",
    "    pair_df = pair_df.groupby(\"aid_x\").head(1)[[\"aid_x\", \"aid_y\"]]\n",
    "    pair_dict = {k: v for k, v in zip(pair_df[\"aid_x\"].tolist(), pair_df[\"aid_y\"].tolist())}\n",
    "    trans_maps[\"clicks_pair\"] = pair_dict\n",
    "\n",
    "    pair_df = pd.read_pickle(cfg.prep_dir + \"co_visitation_matrix_carts.pkl\")\n",
    "    pair_df = pair_df[pair_df[\"cnt\"] > 3].copy()\n",
    "    pair_df = pair_df.sort_values([\"aid_x\", \"cnt\"], ascending=(True, False))\n",
    "    pair_df = pair_df.groupby(\"aid_x\").head(1)[[\"aid_x\", \"aid_y\"]]\n",
    "    pair_dict = {k: v for k, v in zip(pair_df[\"aid_x\"].tolist(), pair_df[\"aid_y\"].tolist())}\n",
    "    trans_maps[\"carts_pair\"] = pair_dict\n",
    "\n",
    "    pair_df = pd.read_pickle(cfg.prep_dir + \"co_visitation_matrix_orders.pkl\")\n",
    "    pair_df = pair_df[pair_df[\"cnt\"] > 3].copy()\n",
    "    pair_df = pair_df.sort_values([\"aid_x\", \"cnt\"], ascending=(True, False))\n",
    "    pair_df = pair_df.groupby(\"aid_x\").head(1)[[\"aid_x\", \"aid_y\"]]\n",
    "    pair_dict = {k: v for k, v in zip(pair_df[\"aid_x\"].tolist(), pair_df[\"aid_y\"].tolist())}\n",
    "    trans_maps[\"orders_pair\"] = pair_dict\n",
    "\n",
    "    pair_df_org = pd.read_pickle(cfg.prep_dir + \"co_visitation_matrix_time_weighted_v2.pkl\")\n",
    "    pair_df_org = pair_df_org.sort_values([\"aid_x\", \"wt\"], ascending=(True, False))\n",
    "    for i in range(10):\n",
    "        pair_df = pair_df_org.groupby(\"aid_x\").nth(i).reset_index()[[\"aid_x\", \"aid_y\"]]\n",
    "        pair_dict = {k: v for k, v in zip(pair_df[\"aid_x\"].tolist(), pair_df[\"aid_y\"].tolist())}\n",
    "        trans_maps[f\"time_wt_pair{str(i)}\"] = pair_dict\n",
    "\n",
    "    pair_df = pd.read_pickle(cfg.prep_dir + \"co_visitation_matrix_type_weighted.pkl\")\n",
    "    pair_df = pair_df.sort_values([\"aid_x\", \"wt\"], ascending=(True, False))\n",
    "    pair_df = pair_df.groupby(\"aid_x\").head(1)[[\"aid_x\", \"aid_y\"]]\n",
    "    pair_dict = {k: v for k, v in zip(pair_df[\"aid_x\"].tolist(), pair_df[\"aid_y\"].tolist())}\n",
    "    trans_maps[\"type_wt_pair\"] = pair_dict\n",
    "\n",
    "    pair_df = pd.read_pickle(cfg.prep_dir + \"co_visitation_matrix_clicks2carts.pkl\")\n",
    "    pair_df = pair_df.sort_values([\"aid_x\", \"cnt\"], ascending=(True, False))\n",
    "    pair_df = pair_df.groupby(\"aid_x\").head(1)[[\"aid_x\", \"aid_y\"]]\n",
    "    pair_dict = {k: v for k, v in zip(pair_df[\"aid_x\"].tolist(), pair_df[\"aid_y\"].tolist())}\n",
    "    trans_maps[\"clicks2carts_pair\"] = pair_dict\n",
    "\n",
    "    pair_df = pd.read_pickle(cfg.prep_dir + \"co_visitation_matrix_clicks2orders.pkl\")\n",
    "    pair_df = pair_df.sort_values([\"aid_x\", \"cnt\"], ascending=(True, False))\n",
    "    pair_df = pair_df.groupby(\"aid_x\").head(1)[[\"aid_x\", \"aid_y\"]]\n",
    "    pair_dict = {k: v for k, v in zip(pair_df[\"aid_x\"].tolist(), pair_df[\"aid_y\"].tolist())}\n",
    "    trans_maps[\"clicks2orders_pair\"] = pair_dict\n",
    "\n",
    "    return trans_maps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cache:\n",
    "    @noglobal(excepts=[\"cfg\", \"logger\"])\n",
    "    def __init__(self):\n",
    "        self.cache_dir = (os.path.join(cfg.output_dir, cfg.exp_name, \"cache\"))\n",
    "        self.cache_dir_path = pathlib.Path(self.cache_dir)\n",
    "        self.caches = list(self.cache_dir_path.glob(\"*.pkl\"))\n",
    "\n",
    "    @noglobal(excepts=[\"cfg\", \"logger\"])\n",
    "    def get_abspath(self, filename):\n",
    "        return (os.path.join(self.cache_dir, f\"{cfg.exp_name}_{filename}\"))\n",
    "    \n",
    "    @noglobal(excepts=[\"cfg\", \"logger\"])\n",
    "    def exists(self, path):\n",
    "        return len([str(c) for c in self.caches if path == str(c)]) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@noglobal(excepts=[\"cfg\", \"logger\"])\n",
    "def make_valid_data(trans_maps):\n",
    "    valid_week = cfg.valid_week\n",
    "    dfs = []\n",
    "    \n",
    "    logger.info(f\"[make_valid] valid_week : {valid_week}\")\n",
    "    # データ読み込み\n",
    "    logger.info(\"[make_valid] read_data : start\")\n",
    "    week_sessions = pd.read_pickle(os.getenv(\"PREP_DIR\") + f\"train_sessions_{valid_week}.pkl\")\n",
    "    week_labels = pd.read_pickle(os.getenv(\"PREP_DIR\") + f\"labels_{valid_week}.pkl\")\n",
    "    valid_session_ids = week_sessions[\"session\"].unique().tolist()\n",
    "    logger.info(\"[make_valid] read_data : end\")\n",
    "\n",
    "    # validに使うsessionを絞る (trainに存在するsessionを除外した上で設定した数に絞る)\n",
    "    logger.info(\"[make_valid] valid_session_extract : start\")\n",
    "    valid_session_n = min(cfg.valid_session_n, len(valid_session_ids))\n",
    "    random.seed(cfg.seed)\n",
    "    valid_session_ids = random.sample(valid_session_ids, valid_session_n)\n",
    "    week_sessions = week_sessions[week_sessions[\"session\"].isin(valid_session_ids)].copy()\n",
    "    week_labels = week_labels[week_labels[\"session\"].isin(valid_session_ids)].copy()\n",
    "    logger.info(\"[make_valid] valid_session_extract : end\")\n",
    "\n",
    "    # 候補選出\n",
    "    logger.info(\"[make_valid] make_cand_df : start\")\n",
    "    valid, results = make_cand_df(week_sessions, labels=week_labels, trans_maps=trans_maps)\n",
    "\n",
    "    return valid, week_labels, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# データ読み込み\n",
    "train_sessions = pd.read_pickle(cfg.prep_dir + \"train_sessions.pkl\")\n",
    "test_sessions = pd.read_pickle(cfg.prep_dir + \"test_sessions.pkl\")\n",
    "\n",
    "train_sessions = train_sessions.drop(columns=[\"type\", \"ts\"])\n",
    "test_sessions = test_sessions.drop(columns=[\"type\", \"ts\"])\n",
    "\n",
    "sessions = pd.concat([train_sessions, test_sessions])\n",
    "sentences = sessions.groupby(\"session\")[\"aid\"].apply(list).tolist()\n",
    "aids = sorted(sessions[\"aid\"].unique().tolist())\n",
    "\n",
    "del train_sessions, test_sessions\n",
    "gc.collect()\n",
    "\n",
    "sentences = sessions.groupby(\"session\")[\"aid\"].apply(list).tolist()\n",
    "aids = sorted(sessions[\"aid\"].unique().tolist())\n",
    "\n",
    "del sessions\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "class LossLogger(CallbackAny2Vec):\n",
    "    '''Output loss at each epoch'''\n",
    "    def __init__(self):\n",
    "        self.epoch = 1\n",
    "        self.losses = []\n",
    "\n",
    "    def on_epoch_begin(self, model):\n",
    "        print(f'Epoch: {self.epoch}', end='\\t')\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        self.losses.append(loss)\n",
    "        print(f'  Loss: {loss}')\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:19:31.034741 \t sample : 0\n",
      "100%|██████████| 1855603/1855603 [09:40<00:00, 3198.37it/s]\n",
      "22:48:40.918249 \t [make_valid] valid_week : week4\n",
      "22:48:40.919657 \t [make_valid] read_data : start\n",
      "22:48:46.250190 \t [make_valid] read_data : end\n",
      "22:48:46.251583 \t [make_valid] valid_session_extract : start\n",
      "22:48:47.240132 \t [make_valid] valid_session_extract : end\n",
      "22:48:47.241521 \t [make_valid] make_cand_df : start\n",
      "22:48:47.296068 \t [make_candidate] session_latest : start\n",
      "22:48:47.614742 \t [add_candidate] session_latest : start\n",
      "22:48:49.192574 \t {'num_clicks': 97136, 'hit_clicks': 29497, 'num_carts': 41843, 'hit_carts': 10523, 'num_orders': 19599, 'hit_orders': 9242, 'recall_clicks': '0.304', 'recall_carts': '0.251', 'recall_orders': '0.472', 'w_recall_clicks': '0.030', 'w_recall_carts': '0.075', 'w_recall_orders': '0.283', 'score': '0.389'}\n",
      "22:48:49.202884 \t [make_candidate] word2vec_sim_pair : start\n",
      "22:48:50.107870 \t [add_candidate] word2vec_sim_pair : start\n",
      "22:48:51.659621 \t {'num_clicks': 97136, 'hit_clicks': 828, 'num_carts': 41843, 'hit_carts': 313, 'num_orders': 19599, 'hit_orders': 213, 'recall_clicks': '0.009', 'recall_carts': '0.007', 'recall_orders': '0.011', 'w_recall_clicks': '0.001', 'w_recall_carts': '0.002', 'w_recall_orders': '0.007', 'score': '0.010'}\n",
      "22:48:53.765836 \t session_cnt_max\n",
      "22:48:53.767029 \t 20\n",
      "22:48:53.767786 \t all_pred_results\n",
      "22:48:53.768621 \t {'num_clicks': 97136, 'hit_clicks': 30066, 'num_carts': 43211, 'hit_carts': 10712, 'num_orders': 20082, 'hit_orders': 9303, 'recall_clicks': '0.310', 'recall_carts': '0.248', 'recall_orders': '0.463', 'w_recall_clicks': '0.031', 'w_recall_carts': '0.074', 'w_recall_orders': '0.278', 'score': '0.383'}\n"
     ]
    }
   ],
   "source": [
    "trans_maps = {}\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "for n in [0]:\n",
    "    logger.info(f\"sample : {str(n)}\")\n",
    "    w2v = Word2Vec(sentences=sentences, vector_size=50, min_count=1, window=20, workers=cores-1, seed=cfg.seed, sg=1, epochs=4, sample=0)\n",
    "    annoy_index = AnnoyIndexer(w2v, 50)\n",
    "    aid_xs = []\n",
    "    aid_ys = []\n",
    "    sims = []\n",
    " \n",
    "    for aid in tqdm(aids):\n",
    "        mss = w2v.wv.most_similar(positive=[aid], topn=21, indexer=annoy_index)\n",
    "        aid_xs.extend([aid for _ in range(20)])\n",
    "        aid_ys.extend([ms[0] for ms in mss][1:])\n",
    "        sims.extend([ms[1] for ms in mss][1:])\n",
    "    trans_maps[\"word2vec_sim_pair\"] = {aid_x:aid_y for aid_x, aid_y in zip(aid_xs, aid_ys)} \n",
    "\n",
    "    valid, valid_labels, results = make_valid_data(trans_maps)\n",
    "    # 予測値を全て使えたときのrecall\n",
    "    valid_all = valid.drop_duplicates(subset= [\"session\", \"aid\"])\n",
    "    session_cnt_max = valid_all[\"session\"].value_counts().max()\n",
    "    valid_all = valid_all.groupby(\"session\")[\"aid\"].apply(list).reset_index()\n",
    "\n",
    "    all_results = evaluate(valid_labels[\"clicks_labels\"].tolist(),\n",
    "                        valid_labels[\"carts_labels\"].tolist(),\n",
    "                        valid_labels[\"orders_labels\"].tolist(),\n",
    "                        valid_all[\"aid\"].tolist(),\n",
    "                        valid_all[\"aid\"].tolist(),\n",
    "                        valid_all[\"aid\"].tolist(),\n",
    "                        session_cnt_max)\n",
    "    logger.info(\"session_cnt_max\")\n",
    "    logger.info(session_cnt_max)\n",
    "    logger.info(\"all_pred_results\")\n",
    "    logger.info(all_results)\n",
    "\n",
    "    report = f\"\\n sample : {n}\\n\"\n",
    "    report += f'session_cnt_max : {str(session_cnt_max)}\\n'\n",
    "    report += f'{str(results[results[\"name\"]==\"word2vec_sim_pair\"])}\\n'\n",
    "    report += str(all_results)\n",
    "    line_notify.send(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:04:10) \n[GCC 10.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
