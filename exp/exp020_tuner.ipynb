{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exp020_tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import random\n",
    "import pathlib\n",
    "import subprocess\n",
    "from dataclasses import dataclass\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv\n",
    "sys.path.append(os.getenv('UTILS_PATH'))\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "import inspect\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import cudf\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import line_notify\n",
    "import optuna.integration.lightgbm as optuna_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import builtins\n",
    "import types\n",
    "\n",
    "def imports():\n",
    "    for name, val in globals().items():\n",
    "        # module imports\n",
    "        if isinstance(val, types.ModuleType):\n",
    "            yield name, val\n",
    "\n",
    "            # functions / callables\n",
    "        if hasattr(val, '__call__'):\n",
    "            yield name, val\n",
    "\n",
    "\n",
    "def noglobal(f):\n",
    "    '''\n",
    "    ref: https://gist.github.com/raven38/4e4c3c7a179283c441f575d6e375510c\n",
    "    '''\n",
    "    return types.FunctionType(f.__code__,\n",
    "                              dict(imports()),\n",
    "                              f.__name__,\n",
    "                              f.__defaults__,\n",
    "                              f.__closure__\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Cfg:\n",
    "    exp_name = \"exp020\"\n",
    "    seed = 42\n",
    "    k = 20\n",
    "    type2id = {\"clicks\":0, \"carts\":1, \"orders\":2}\n",
    "    id2type = {0:\"clicks\", 1:\"carts\", 2:\"orders\"}\n",
    "    train_weeks = [\"week3\"]\n",
    "    valid_week = \"week4\"\n",
    "    valid_session_n = 1_000_000\n",
    "    input_dir = os.getenv('INPUT_DIR')\n",
    "    output_dir = os.getenv('OUTPUT_DIR')\n",
    "    prep_dir = os.getenv(\"PREP_DIR\")\n",
    "\n",
    "cfg = Cfg()\n",
    "# os.makedirs(os.path.join(cfg.output_dir, cfg.exp_name), exist_ok=True)\n",
    "random.seed(cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@noglobal\n",
    "def evaluate(clicks_labels, carts_labels, orders_labels, \n",
    "             clicks_preds, carts_preds, orders_preds, k=20):\n",
    "\n",
    "    num_clicks = 0\n",
    "    num_carts = 0\n",
    "    num_orders = 0\n",
    "    hit_clicks = 0\n",
    "    hit_carts = 0\n",
    "    hit_orders = 0\n",
    "\n",
    "    for i in range(len(clicks_labels)):\n",
    "        clicks_label = clicks_labels[i]\n",
    "        carts_label = carts_labels[i]\n",
    "        orders_label = orders_labels[i]\n",
    "        clicks_pred = clicks_preds[i]\n",
    "        carts_pred = carts_preds[i]\n",
    "        orders_pred = orders_preds[i]\n",
    "\n",
    "        if type(clicks_pred) == list:\n",
    "            clicks_pred = clicks_pred[:k]\n",
    "        else:\n",
    "            clicks_pred = []\n",
    "        if type(carts_pred) == list:\n",
    "            carts_pred = carts_pred[:k]\n",
    "        else:\n",
    "            carts_pred = []    \n",
    "        if type(orders_pred) == list:\n",
    "            orders_pred = orders_pred[:k]\n",
    "        else:\n",
    "            orders_pred = []\n",
    "\n",
    "        if not np.isnan(clicks_label):\n",
    "            num_clicks += 1\n",
    "            hit_clicks += int(clicks_label in clicks_pred)\n",
    "\n",
    "        if type(carts_label) == list:\n",
    "            num_carts += min(len(carts_label), k)\n",
    "            hit_carts += len(set(carts_pred) & set(carts_label))\n",
    "            \n",
    "        if type(orders_label) == list:\n",
    "            num_orders += min(len(orders_label), k)\n",
    "            hit_orders += len(set(orders_pred) & set(orders_label))\n",
    "\n",
    "\n",
    "    recall_clicks = hit_clicks / num_clicks\n",
    "    recall_carts = hit_carts / num_carts\n",
    "    recall_orders = hit_orders / num_orders\n",
    "    w_recall_clicks = recall_clicks * 0.10\n",
    "    w_recall_carts = recall_carts * 0.30\n",
    "    w_recall_orders = recall_orders * 0.6\n",
    "    score = w_recall_clicks + w_recall_carts + w_recall_orders\n",
    "\n",
    "    results = {}\n",
    "    results[\"num_clicks\"] = num_clicks\n",
    "    results[\"hit_clicks\"] = hit_clicks\n",
    "    results[\"num_carts\"] = num_carts\n",
    "    results[\"hit_carts\"] = hit_carts\n",
    "    results[\"num_orders\"] = num_orders\n",
    "    results[\"hit_orders\"] = hit_orders\n",
    "    results[\"recall_clicks\"] = format(recall_clicks, \".3f\")\n",
    "    results[\"recall_carts\"] = format(recall_carts, \".3f\")\n",
    "    results[\"recall_orders\"] = format(recall_orders, \".3f\")\n",
    "    results[\"w_recall_clicks\"] = format(w_recall_clicks, \".3f\")\n",
    "    results[\"w_recall_carts\"] = format(w_recall_carts, \".3f\")\n",
    "    results[\"w_recall_orders\"] = format(w_recall_orders, \".3f\")\n",
    "    results[\"score\"] = format(score, \".3f\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Candidate:\n",
    "    def __init__(self, pdf, labels=None):\n",
    "        self.df = cudf.from_pandas(pdf)\n",
    "        self.target_sessions = pdf[\"session\"].unique().tolist()\n",
    "        self.results = pd.DataFrame(columns=[\"name\", \"num_clicks\", \"hit_clicks\", \"num_carts\", \"hit_carts\", \"num_orders\", \"hit_orders\", \n",
    "                                             \"recall_clicks\", \"recall_carts\", \"recall_orders\", \"w_recall_clicks\", \"w_recall_carts\", \"w_recall_orders\", \"score\"])\n",
    "        self.output = pd.DataFrame(columns=[\"session\", \"aid\"], dtype=int)\n",
    "        self.labels = labels\n",
    "\n",
    "    def _entry(self, new_candidate_df, name, k):\n",
    "        new_candidate_df[f\"{name}_rank\"] = new_candidate_df.groupby(\"session\")[\"session\"].cumcount()\n",
    "        new_candidate_df = new_candidate_df.to_pandas()\n",
    "        self.output = pd.concat([self.output, new_candidate_df[[\"session\", \"aid\"]]])\n",
    "        self.output = self.output.drop_duplicates(subset=[\"session\", \"aid\"])\n",
    "        \n",
    "        self.output = self.output.merge(new_candidate_df, on=[\"session\", \"aid\"], how=\"left\")\n",
    "\n",
    "        if self.labels is not None:\n",
    "            self._eval(new_candidate_df[[\"session\", \"aid\"]], name, k)\n",
    "\n",
    "    def _eval(self, new_candidate_df, name, k):\n",
    "        new_candidate_df = new_candidate_df.groupby(\"session\")[\"aid\"].apply(list).reset_index()\n",
    "        eval_df = pd.DataFrame(self.target_sessions, columns=[\"session\"])\n",
    "        eval_df = eval_df.merge(new_candidate_df, on=[\"session\"], how=\"left\")\n",
    "        assert eval_df[\"session\"].tolist() == self.labels[\"session\"].tolist()\n",
    "        eval_result = evaluate(self.labels[\"clicks_labels\"].tolist(), self.labels[\"carts_labels\"].tolist(), self.labels[\"orders_labels\"].tolist(),\n",
    "                               eval_df[\"aid\"].tolist(), eval_df[\"aid\"].tolist(), eval_df[\"aid\"].tolist(), k)\n",
    "        print(name)\n",
    "        print(eval_result)\n",
    "        self.results = pd.concat([self.results, pd.DataFrame([[name] + list(eval_result.values())], columns=[\"name\"] + list(eval_result.keys()))])\n",
    "\n",
    "    def add(self, strategy, name, usetypes=['clicks', 'carts', 'orders'], trans_map=None, k=20):\n",
    "        candidate_df = self.df[self.df[\"type\"].isin(usetypes)].copy()\n",
    "        \n",
    "        if strategy == \"session_frequent\":\n",
    "            candidate_df = self._session_frequent(candidate_df, k)\n",
    "        elif strategy == \"session_latest\":\n",
    "            candidate_df = self._session_latest(candidate_df, k)\n",
    "        elif strategy == \"total_frequent\":\n",
    "            candidate_df = self._total_frequent(candidate_df, k)\n",
    "        \n",
    "        if trans_map is not None:\n",
    "            candidate_df[\"aid\"] = candidate_df[\"aid\"].map(trans_map)\n",
    "            candidate_df = candidate_df.dropna(subset=[\"aid\"])\n",
    "            \n",
    "        self._entry(candidate_df, name, k)\n",
    "\n",
    "    def _session_frequent(self, candidate_df, k):\n",
    "        candidate_df = candidate_df.groupby([\"session\", \"aid\"])[\"ts\"].count().reset_index()\n",
    "        candidate_df.columns = [\"session\", \"aid\", \"aid_count\"]\n",
    "        candidate_df = candidate_df.sort_values([\"session\", \"aid_count\", \"aid\"], ascending=(True, False, True))\n",
    "        candidate_df = candidate_df[candidate_df.groupby(\"session\")[\"aid_count\"].cumcount() < k].copy()\n",
    "        candidate_df = candidate_df[[\"session\", \"aid\"]].copy()\n",
    "        return candidate_df\n",
    "\n",
    "    def _session_latest(self, candidate_df, k):\n",
    "        candidate_df = candidate_df.sort_values([\"session\", \"ts\"], ascending=(True, False)).drop_duplicates(subset=[\"session\", \"aid\"])\n",
    "        candidate_df = candidate_df.sort_values([\"session\", \"ts\"], ascending=(True, False))[candidate_df.groupby(\"session\")[\"ts\"].cumcount() < k].copy()\n",
    "        candidate_df = candidate_df[[\"session\", \"aid\"]].copy()\n",
    "        return candidate_df\n",
    "\n",
    "    def _total_frequent(self, candidate_df, k):\n",
    "        candidate_df = candidate_df.groupby(\"aid\")[\"ts\"].count().reset_index()\n",
    "        candidate_df.columns = [\"aid\", \"aid_count\"]\n",
    "        topk_freq_aids = candidate_df.sort_values(\"aid_count\", ascending=False)[\"aid\"].to_arrow().to_pylist()[:k]\n",
    "        sessions = []\n",
    "        aids = []\n",
    "        for session, aid in itertools.product(self.target_sessions, topk_freq_aids):\n",
    "            sessions.append(session)\n",
    "            aids.append(aid)\n",
    "        candidate_df = cudf.DataFrame({\"session\": sessions, \"aid\": aids})\n",
    "        return candidate_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@noglobal\n",
    "def add_labels(df, labels, type_):\n",
    "    type_labels = labels[[\"session\", f\"{type_}_labels\"]].dropna().copy()\n",
    "    type_labels.columns = [\"session\", \"aid\"]\n",
    "    type_labels = type_labels.explode(\"aid\")\n",
    "    type_labels[\"labels\"] = 1\n",
    "    df = df.merge(type_labels, on=[\"session\", \"aid\"], how=\"left\")\n",
    "    df[\"labels\"] = df[\"labels\"].fillna(0)\n",
    "    return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@noglobal\n",
    "def add_ground_truth_candidate(df, labels, type_):\n",
    "    gt = labels[[\"session\", f\"{type_}_labels\"]].dropna().copy()\n",
    "    gt.columns = [\"session\", \"aid\"]\n",
    "    gt = gt.explode(\"aid\")\n",
    "    df = pd.concat([df, gt])\n",
    "    df = df.drop_duplicates()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@noglobal\n",
    "def under_sampling(df, label_col):\n",
    "    cfg = Cfg()\n",
    "    pos_df = df[df[label_col]==1].copy()\n",
    "    neg_df = df[df[label_col]==0].copy()\n",
    "    pos_n = len(pos_df)\n",
    "    return pd.concat([pos_df, neg_df.sample(pos_n, random_state=cfg.seed)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddFeatures:\n",
    "    def __init__(self, sessions, candidate):\n",
    "        cfg = Cfg()\n",
    "        self.sessions = cudf.from_pandas(sessions)\n",
    "        self.output = candidate\n",
    "        self.type_dict = cfg.type2id\n",
    "\n",
    "    def add_features(self, features_name):\n",
    "        if features_name == \"session_cnt\":\n",
    "            self._session_cnt()\n",
    "        elif features_name == \"session_aid_nunique\":\n",
    "            self._session_aid_nunique()\n",
    "        elif features_name == \"session_last_type\":\n",
    "            self._session_last_type()\n",
    "        elif features_name == \"aid_cnt\":\n",
    "            self._aid_cnt()\n",
    "        \n",
    "    def _session_cnt(self):\n",
    "        agg_df = self.sessions.groupby([\"session\", \"type\"])[\"ts\"].count().reset_index().rename(columns={\"ts\": \"cnt\"})\n",
    "        # session_total_cnt\n",
    "        features = agg_df.groupby(\"session\")[\"cnt\"].sum().reset_index().rename(columns={\"cnt\": \"session_total_cnt\"})\n",
    "        self.output = self.output.merge(features.to_pandas(), on=[\"session\"], how=\"left\")\n",
    "        self.output[\"session_total_cnt\"] = self.output[\"session_total_cnt\"].fillna(0)\n",
    "        # session_{type}_cnt\n",
    "        for type_ in [\"clicks\", \"carts\", \"orders\"]:\n",
    "            col_name = f\"session_{type_}_cnt\"\n",
    "            features = agg_df[agg_df[\"type\"]==type_].copy()\n",
    "            features = features.rename(columns={\"cnt\": col_name})\n",
    "            features = features[[\"session\", col_name]].copy()\n",
    "            self.output = self.output.merge(features.to_pandas(), on=[\"session\"], how=\"left\")\n",
    "            self.output[col_name] = self.output[col_name].fillna(0)\n",
    "\n",
    "    def _session_aid_nunique(self):\n",
    "        features = self.sessions.groupby(\"session\")[\"aid\"].nunique().reset_index().rename(columns={\"aid\": \"session_aid_nunique\"})\n",
    "        self.output = self.output.merge(features.to_pandas(), on=[\"session\"], how=\"left\")\n",
    "        self.output[\"session_aid_nunique\"] = self.output[\"session_aid_nunique\"].fillna(0)\n",
    "        \n",
    "    def _session_last_type(self):\n",
    "        features = self.sessions.groupby(\"session\").nth(-1).reset_index()[[\"session\", \"type\"]]\n",
    "        features = features.rename(columns={\"type\": \"session_last_type\"})\n",
    "        features[\"session_last_type\"] = features[\"session_last_type\"].map(self.type_dict)\n",
    "        self.output = self.output.merge(features.to_pandas(), on=[\"session\"], how=\"left\")\n",
    "\n",
    "    def _aid_cnt(self):\n",
    "        agg_df = self.sessions.groupby([\"session\", \"aid\", \"type\"])[\"ts\"].count().reset_index().rename(columns={\"ts\": \"cnt\"})\n",
    "\n",
    "        # aid_total_cnt\n",
    "        features = agg_df.groupby([\"session\", \"aid\"])[\"cnt\"].sum().reset_index().rename(columns={\"cnt\": \"aid_total_cnt\"})\n",
    "        self.output = self.output.merge(features.to_pandas(), on=[\"session\", \"aid\"], how=\"left\")\n",
    "        self.output[\"aid_total_cnt\"] = self.output[\"aid_total_cnt\"].fillna(0)\n",
    "\n",
    "        # aid_{type}_cnt\n",
    "        for type_ in [\"clicks\", \"carts\", \"orders\"]:\n",
    "            col_name = f\"aid_{type_}_cnt\"\n",
    "            features = agg_df[agg_df[\"type\"]==type_].copy()\n",
    "            features = features.rename(columns={\"cnt\": col_name})\n",
    "            features = features[[\"session\", \"aid\", col_name]].copy()\n",
    "            self.output = self.output.merge(features.to_pandas(), on=[\"session\", \"aid\"], how=\"left\")\n",
    "            self.output[col_name] = self.output[col_name].fillna(0)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@noglobal\n",
    "def make_cand_df(sessions, labels=None, trans_maps=None):\n",
    "    cfg = Cfg()\n",
    "\n",
    "    if labels is not None:\n",
    "         cand = Candidate(sessions)\n",
    "    else:\n",
    "        cand = Candidate(sessions, labels)\n",
    "\n",
    "    cand.add(strategy=\"session_frequent\", name=\"session_frequent\")\n",
    "    cand.add(strategy=\"session_latest\", name=\"session_latest\")\n",
    "    cand.add(strategy=\"total_frequent\", name=\"total_frequent\")\n",
    "    cand.add(strategy=\"session_frequent\", trans_map=trans_maps[\"pair0\"], name=\"session_frequent_pair\")\n",
    "    \n",
    "    for i in range(5):\n",
    "        cand.add(strategy=\"session_latest\", trans_map=trans_maps[f\"pair{str(i)}\"], name=f\"session_latest_pair{str(i)}\")\n",
    "      \n",
    "    cand.add(strategy=\"session_frequent\", trans_map=trans_maps[\"clicks_pair\"], name=\"session_frequent_clicks_pair\")\n",
    "    cand.add(strategy=\"session_latest\", trans_map=trans_maps[\"clicks_pair\"], name=\"session_latest_clicks_pair\")\n",
    "    cand.add(strategy=\"session_latest\", trans_map=trans_maps[\"carts_pair\"], name=\"session_latest_carts_pair\")\n",
    "    return cand.output\n",
    "\n",
    "@noglobal\n",
    "def add_features(sessions, cand_df):\n",
    "    feat = AddFeatures(sessions, cand_df)\n",
    "    feat.add_features(\"session_cnt\")\n",
    "    feat.add_features(\"session_aid_nunique\")\n",
    "    feat.add_features(\"session_last_type\")\n",
    "    feat.add_features(\"aid_cnt\")\n",
    "    return feat.output.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_maps = {}\n",
    "pair_df_org = pd.read_pickle(cfg.prep_dir + \"co_visitation_matrix.pkl\")\n",
    "pair_df_org = pair_df_org[pair_df_org[\"cnt\"] > 3].copy()\n",
    "pair_df_org = pair_df_org.sort_values([\"aid_x\", \"cnt\"], ascending=(True, False))\n",
    "\n",
    "for i in range(5):\n",
    "    pair_df = pair_df_org.groupby(\"aid_x\").nth(i).reset_index()[[\"aid_x\", \"aid_y\"]]\n",
    "    pair_dict = {k: v for k, v in zip(pair_df[\"aid_x\"].tolist(), pair_df[\"aid_y\"].tolist())}\n",
    "    trans_maps[f\"pair{str(i)}\"] = pair_dict\n",
    "\n",
    "pair_df = pd.read_pickle(cfg.prep_dir + \"co_visitation_matrix_clicks.pkl\")\n",
    "pair_df = pair_df[pair_df[\"cnt\"] > 3].copy()\n",
    "pair_df = pair_df.sort_values([\"aid_x\", \"cnt\"], ascending=(True, False))\n",
    "pair_df = pair_df.groupby(\"aid_x\").head(1)[[\"aid_x\", \"aid_y\"]]\n",
    "pair_dict = {k: v for k, v in zip(pair_df[\"aid_x\"].tolist(), pair_df[\"aid_y\"].tolist())}\n",
    "trans_maps[\"clicks_pair\"] = pair_dict\n",
    "\n",
    "pair_df = pd.read_pickle(cfg.prep_dir + \"co_visitation_matrix_carts.pkl\")\n",
    "pair_df = pair_df[pair_df[\"cnt\"] > 3].copy()\n",
    "pair_df = pair_df.sort_values([\"aid_x\", \"cnt\"], ascending=(True, False))\n",
    "pair_df = pair_df.groupby(\"aid_x\").head(1)[[\"aid_x\", \"aid_y\"]]\n",
    "pair_dict = {k: v for k, v in zip(pair_df[\"aid_x\"].tolist(), pair_df[\"aid_y\"].tolist())}\n",
    "trans_maps[\"carts_pair\"] = pair_dict\n",
    "\n",
    "pair_df = pd.read_pickle(cfg.prep_dir + \"co_visitation_matrix_orders.pkl\")\n",
    "pair_df = pair_df[pair_df[\"cnt\"] > 3].copy()\n",
    "pair_df = pair_df.sort_values([\"aid_x\", \"cnt\"], ascending=(True, False))\n",
    "pair_df = pair_df.groupby(\"aid_x\").head(1)[[\"aid_x\", \"aid_y\"]]\n",
    "pair_dict = {k: v for k, v in zip(pair_df[\"aid_x\"].tolist(), pair_df[\"aid_y\"].tolist())}\n",
    "trans_maps[\"orders_pair\"] = pair_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# trainデータ作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = f\"\\n{cfg.exp_name}\\n\"\n",
    "report += f'train_start\\n'\n",
    "line_notify.send(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_weeks = cfg.train_weeks\n",
    "train_session_ids = []\n",
    "dfs = []\n",
    "for weeks in train_weeks:\n",
    "    # データ読み込み\n",
    "    week_sessions = pd.read_pickle(os.getenv(\"PREP_DIR\") + f\"train_sessions_{weeks}.pkl\")\n",
    "    week_labels = pd.read_pickle(os.getenv(\"PREP_DIR\") + f\"labels_{weeks}.pkl\")\n",
    "    train_session_ids.extend(week_sessions[\"session\"].unique().tolist())\n",
    "\n",
    "    for type_, type_int in zip([\"clicks\", \"carts\", \"orders\"], [0, 1, 2]):\n",
    "        # 候補選出\n",
    "        cand_df = make_cand_df(week_sessions, trans_maps=trans_maps)\n",
    "\n",
    "        # 正解追加・under sampling\n",
    "        # cand_df = add_ground_truth_candidate(cand_df, week_labels, type_)\n",
    "        cand_df = add_labels(cand_df, week_labels, type_)\n",
    "        cand_df = under_sampling(cand_df, \"labels\")\n",
    "\n",
    "        # 特徴量付与\n",
    "        train_tmp = add_features(week_sessions, cand_df)\n",
    "        train_tmp[\"type\"] = type_int\n",
    "        del cand_df\n",
    "        gc.collect()\n",
    "        dfs.append(train_tmp)\n",
    "train = pd.concat(dfs)\n",
    "train = train.reset_index(drop=True)\n",
    "train_session_ids = list(set(train_session_ids))\n",
    "\n",
    "del dfs, week_sessions, week_labels\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# validデータ作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_week = cfg.valid_week\n",
    "dfs = []\n",
    "\n",
    "# データ読み込み\n",
    "week_sessions = pd.read_pickle(os.getenv(\"PREP_DIR\") + f\"train_sessions_{valid_week}.pkl\")\n",
    "week_labels = pd.read_pickle(os.getenv(\"PREP_DIR\") + f\"labels_{valid_week}.pkl\")\n",
    "valid_session_ids = week_sessions[\"session\"].unique().tolist()\n",
    "\n",
    "# validに使うsessionを絞る (trainに存在するsessionを除外した上で設定した数に絞る)\n",
    "valid_session_n = min(cfg.valid_session_n, len(valid_session_ids))\n",
    "valid_session_ids = random.sample(valid_session_ids, valid_session_n)\n",
    "week_sessions = week_sessions[week_sessions[\"session\"].isin(valid_session_ids)].copy()\n",
    "week_labels = week_labels[week_labels[\"session\"].isin(valid_session_ids)].copy()\n",
    "\n",
    "for type_, type_int in zip([\"clicks\", \"carts\", \"orders\"], [0, 1, 2]):\n",
    "    # 候補選出\n",
    "    cand_df = make_cand_df(week_sessions, trans_maps=trans_maps)\n",
    "\n",
    "    # ラベル付与\n",
    "    cand_df = add_labels(cand_df, week_labels, type_)\n",
    "    \n",
    "    # 特徴量付与\n",
    "    train_tmp = add_features(week_sessions, cand_df)\n",
    "    train_tmp[\"type\"] = type_int\n",
    "    del cand_df\n",
    "    gc.collect()\n",
    "\n",
    "    dfs.append(train_tmp)\n",
    "valid = pd.concat(dfs)\n",
    "valid = valid.reset_index(drop=True)\n",
    "\n",
    "del dfs, week_sessions\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"labels\"\n",
    "not_use_cols = [\"session\", \"aid\", target]\n",
    "features = [c for c in train.columns if c not in not_use_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "   'objective': 'binary',\n",
    "   'boosting': 'gbdt',\n",
    "   'learning_rate': 0.1,\n",
    "   'metric': 'binary_logloss',\n",
    "   'seed': cfg.seed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-24 10:13:49,213]\u001b[0m A new study created in memory with name: no-name-b33e4239-dff7-4815-b000-e3e6a1512955\u001b[0m\n",
      "feature_fraction, val_score: inf:   0%|          | 0/7 [00:00<?, ?it/s]/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/home/kaggler/.local/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.267990 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.294831\tvalid_1's binary_logloss: 0.297561\n",
      "[2000]\tvalid_0's binary_logloss: 0.292422\tvalid_1's binary_logloss: 0.29706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.296994:  14%|#4        | 1/7 [13:51<1:23:09, 831.54s/it]\u001b[32m[I 2022-11-24 10:27:40,816]\u001b[0m Trial 0 finished with value: 0.29699431414862765 and parameters: {'feature_fraction': 0.6}. Best is trial 0 with value: 0.29699431414862765.\u001b[0m\n",
      "feature_fraction, val_score: 0.296994:  14%|#4        | 1/7 [13:51<1:23:09, 831.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2291]\tvalid_0's binary_logloss: 0.291834\tvalid_1's binary_logloss: 0.296994\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.334738 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.294588\tvalid_1's binary_logloss: 0.297524\n",
      "[2000]\tvalid_0's binary_logloss: 0.292084\tvalid_1's binary_logloss: 0.296989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.296984:  29%|##8       | 2/7 [25:11<1:01:51, 742.26s/it]\u001b[32m[I 2022-11-24 10:39:00,570]\u001b[0m Trial 1 finished with value: 0.29698441772766837 and parameters: {'feature_fraction': 0.8}. Best is trial 1 with value: 0.29698441772766837.\u001b[0m\n",
      "feature_fraction, val_score: 0.296984:  29%|##8       | 2/7 [25:11<1:01:51, 742.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1908]\tvalid_0's binary_logloss: 0.292282\tvalid_1's binary_logloss: 0.296984\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.308931 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.295037\tvalid_1's binary_logloss: 0.297754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.296984:  43%|####2     | 3/7 [36:25<47:24, 711.14s/it]  \u001b[32m[I 2022-11-24 10:50:14,687]\u001b[0m Trial 2 finished with value: 0.2972425505959464 and parameters: {'feature_fraction': 0.5}. Best is trial 1 with value: 0.29698441772766837.\u001b[0m\n",
      "feature_fraction, val_score: 0.296984:  43%|####2     | 3/7 [36:25<47:24, 711.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1792]\tvalid_0's binary_logloss: 0.293057\tvalid_1's binary_logloss: 0.297243\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.376247 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.294525\tvalid_1's binary_logloss: 0.297452\n",
      "[2000]\tvalid_0's binary_logloss: 0.291957\tvalid_1's binary_logloss: 0.297013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.296873:  57%|#####7    | 4/7 [52:04<40:03, 801.27s/it]\u001b[32m[I 2022-11-24 11:05:54,111]\u001b[0m Trial 3 finished with value: 0.2968732104317892 and parameters: {'feature_fraction': 0.8999999999999999}. Best is trial 3 with value: 0.2968732104317892.\u001b[0m\n",
      "feature_fraction, val_score: 0.296873:  57%|#####7    | 4/7 [52:04<40:03, 801.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2830]\tvalid_0's binary_logloss: 0.29023\tvalid_1's binary_logloss: 0.296873\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.336330 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.295388\tvalid_1's binary_logloss: 0.297836\n",
      "[2000]\tvalid_0's binary_logloss: 0.293127\tvalid_1's binary_logloss: 0.297261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.296873:  71%|#######1  | 5/7 [1:05:21<26:39, 799.60s/it]\u001b[32m[I 2022-11-24 11:19:10,757]\u001b[0m Trial 4 finished with value: 0.29712058423208365 and parameters: {'feature_fraction': 0.4}. Best is trial 3 with value: 0.2968732104317892.\u001b[0m\n",
      "feature_fraction, val_score: 0.296873:  71%|#######1  | 5/7 [1:05:21<26:39, 799.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2210]\tvalid_0's binary_logloss: 0.292696\tvalid_1's binary_logloss: 0.297121\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.331350 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.294739\tvalid_1's binary_logloss: 0.297646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.296873:  86%|########5 | 6/7 [1:16:38<12:37, 757.99s/it]\u001b[32m[I 2022-11-24 11:30:27,981]\u001b[0m Trial 5 finished with value: 0.2971016342998251 and parameters: {'feature_fraction': 0.7}. Best is trial 3 with value: 0.2968732104317892.\u001b[0m\n",
      "feature_fraction, val_score: 0.296873:  86%|########5 | 6/7 [1:16:38<12:37, 757.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1871]\tvalid_0's binary_logloss: 0.29251\tvalid_1's binary_logloss: 0.297102\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.363507 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.294443\tvalid_1's binary_logloss: 0.297562\n",
      "[2000]\tvalid_0's binary_logloss: 0.291846\tvalid_1's binary_logloss: 0.297007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction, val_score: 0.296873: 100%|##########| 7/7 [1:27:48<00:00, 729.08s/it]\u001b[32m[I 2022-11-24 11:41:37,523]\u001b[0m Trial 6 finished with value: 0.2969964880751537 and parameters: {'feature_fraction': 1.0}. Best is trial 3 with value: 0.2968732104317892.\u001b[0m\n",
      "feature_fraction, val_score: 0.296873: 100%|##########| 7/7 [1:27:48<00:00, 752.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1916]\tvalid_0's binary_logloss: 0.292024\tvalid_1's binary_logloss: 0.296996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.296873:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.367273 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.296873:   5%|5         | 1/20 [03:11<1:00:31, 191.11s/it]\u001b[32m[I 2022-11-24 11:44:48,649]\u001b[0m Trial 7 finished with value: 0.33485745604448 and parameters: {'num_leaves': 2}. Best is trial 7 with value: 0.33485745604448.\u001b[0m\n",
      "num_leaves, val_score: 0.296873:   5%|5         | 1/20 [03:11<1:00:31, 191.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[583]\tvalid_0's binary_logloss: 0.328214\tvalid_1's binary_logloss: 0.334857\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.382443 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.296873:  10%|#         | 2/20 [09:43<1:32:46, 309.24s/it]\u001b[32m[I 2022-11-24 11:51:20,575]\u001b[0m Trial 8 finished with value: 0.2970197878526449 and parameters: {'num_leaves': 203}. Best is trial 8 with value: 0.2970197878526449.\u001b[0m\n",
      "num_leaves, val_score: 0.296873:  10%|#         | 2/20 [09:43<1:32:46, 309.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[570]\tvalid_0's binary_logloss: 0.288237\tvalid_1's binary_logloss: 0.29702\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.356743 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.293786\tvalid_1's binary_logloss: 0.297275\n",
      "[2000]\tvalid_0's binary_logloss: 0.290887\tvalid_1's binary_logloss: 0.296901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.296873:  15%|#5        | 3/20 [22:30<2:26:54, 518.51s/it]\u001b[32m[I 2022-11-24 12:04:08,131]\u001b[0m Trial 9 finished with value: 0.29687978855629615 and parameters: {'num_leaves': 38}. Best is trial 9 with value: 0.29687978855629615.\u001b[0m\n",
      "num_leaves, val_score: 0.296873:  15%|#5        | 3/20 [22:30<2:26:54, 518.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2146]\tvalid_0's binary_logloss: 0.29052\tvalid_1's binary_logloss: 0.29688\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.321076 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.291465\tvalid_1's binary_logloss: 0.297073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.296873:  20%|##        | 4/20 [30:58<2:17:07, 514.21s/it]\u001b[32m[I 2022-11-24 12:12:35,740]\u001b[0m Trial 10 finished with value: 0.2969879745370658 and parameters: {'num_leaves': 67}. Best is trial 9 with value: 0.29687978855629615.\u001b[0m\n",
      "num_leaves, val_score: 0.296873:  20%|##        | 4/20 [30:58<2:17:07, 514.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1218]\tvalid_0's binary_logloss: 0.290442\tvalid_1's binary_logloss: 0.296988\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.323179 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.288835\tvalid_1's binary_logloss: 0.29698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.296873:  25%|##5       | 5/20 [38:52<2:04:55, 499.70s/it]\u001b[32m[I 2022-11-24 12:20:29,703]\u001b[0m Trial 11 finished with value: 0.2969767942986101 and parameters: {'num_leaves': 105}. Best is trial 9 with value: 0.29687978855629615.\u001b[0m\n",
      "num_leaves, val_score: 0.296873:  25%|##5       | 5/20 [38:52<2:04:55, 499.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[977]\tvalid_0's binary_logloss: 0.288972\tvalid_1's binary_logloss: 0.296977\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.362139 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.293575\tvalid_1's binary_logloss: 0.297312\n",
      "[2000]\tvalid_0's binary_logloss: 0.290531\tvalid_1's binary_logloss: 0.296928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.296873:  30%|###       | 6/20 [50:49<2:13:53, 573.86s/it]\u001b[32m[I 2022-11-24 12:32:27,514]\u001b[0m Trial 12 finished with value: 0.2969236585126455 and parameters: {'num_leaves': 41}. Best is trial 9 with value: 0.29687978855629615.\u001b[0m\n",
      "num_leaves, val_score: 0.296873:  30%|###       | 6/20 [50:49<2:13:53, 573.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1947]\tvalid_0's binary_logloss: 0.290682\tvalid_1's binary_logloss: 0.296924\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.346152 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.296873:  35%|###5      | 7/20 [57:10<1:50:40, 510.79s/it]\u001b[32m[I 2022-11-24 12:38:48,469]\u001b[0m Trial 13 finished with value: 0.297075520642719 and parameters: {'num_leaves': 236}. Best is trial 9 with value: 0.29687978855629615.\u001b[0m\n",
      "num_leaves, val_score: 0.296873:  35%|###5      | 7/20 [57:10<1:50:40, 510.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[487]\tvalid_0's binary_logloss: 0.288252\tvalid_1's binary_logloss: 0.297076\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.400619 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.296873:  40%|####      | 8/20 [1:03:57<1:35:29, 477.49s/it]\u001b[32m[I 2022-11-24 12:45:34,648]\u001b[0m Trial 14 finished with value: 0.29702226999329 and parameters: {'num_leaves': 144}. Best is trial 9 with value: 0.29687978855629615.\u001b[0m\n",
      "num_leaves, val_score: 0.296873:  40%|####      | 8/20 [1:03:57<1:35:29, 477.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[722]\tvalid_0's binary_logloss: 0.288909\tvalid_1's binary_logloss: 0.297022\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.320454 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.296873:  45%|####5     | 9/20 [1:09:17<1:18:30, 428.23s/it]\u001b[32m[I 2022-11-24 12:50:54,578]\u001b[0m Trial 15 finished with value: 0.2971266579014534 and parameters: {'num_leaves': 190}. Best is trial 9 with value: 0.29687978855629615.\u001b[0m\n",
      "num_leaves, val_score: 0.296873:  45%|####5     | 9/20 [1:09:17<1:18:30, 428.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[446]\tvalid_0's binary_logloss: 0.290294\tvalid_1's binary_logloss: 0.297127\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.334292 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.296873:  50%|#####     | 10/20 [1:15:03<1:07:09, 402.91s/it]\u001b[32m[I 2022-11-24 12:56:40,792]\u001b[0m Trial 16 finished with value: 0.297135412220841 and parameters: {'num_leaves': 205}. Best is trial 9 with value: 0.29687978855629615.\u001b[0m\n",
      "num_leaves, val_score: 0.296873:  50%|#####     | 10/20 [1:15:03<1:07:09, 402.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[500]\tvalid_0's binary_logloss: 0.289098\tvalid_1's binary_logloss: 0.297135\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.348834 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.30433\tvalid_1's binary_logloss: 0.307238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.296873:  55%|#####5    | 11/20 [1:19:56<55:23, 369.25s/it]  \u001b[32m[I 2022-11-24 13:01:33,705]\u001b[0m Trial 17 finished with value: 0.30685404136499494 and parameters: {'num_leaves': 3}. Best is trial 9 with value: 0.29687978855629615.\u001b[0m\n",
      "num_leaves, val_score: 0.296873:  55%|#####5    | 11/20 [1:19:56<55:23, 369.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[974]\tvalid_0's binary_logloss: 0.304447\tvalid_1's binary_logloss: 0.306854\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.380699 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.292306\tvalid_1's binary_logloss: 0.297151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.296869:  60%|######    | 12/20 [1:30:12<59:16, 444.51s/it]\u001b[32m[I 2022-11-24 13:11:50,356]\u001b[0m Trial 18 finished with value: 0.296868724263304 and parameters: {'num_leaves': 56}. Best is trial 18 with value: 0.296868724263304.\u001b[0m\n",
      "num_leaves, val_score: 0.296869:  60%|######    | 12/20 [1:30:12<59:16, 444.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1615]\tvalid_0's binary_logloss: 0.289935\tvalid_1's binary_logloss: 0.296869\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.344152 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.290531\tvalid_1's binary_logloss: 0.297074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.296869:  65%|######5   | 13/20 [1:38:32<53:48, 461.26s/it]\u001b[32m[I 2022-11-24 13:20:10,159]\u001b[0m Trial 19 finished with value: 0.2970110732784132 and parameters: {'num_leaves': 80}. Best is trial 18 with value: 0.296868724263304.\u001b[0m\n",
      "num_leaves, val_score: 0.296869:  65%|######5   | 13/20 [1:38:32<53:48, 461.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1166]\tvalid_0's binary_logloss: 0.289645\tvalid_1's binary_logloss: 0.297011\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.346290 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.292696\tvalid_1's binary_logloss: 0.297234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.296869:  70%|#######   | 14/20 [1:49:16<51:37, 516.32s/it]\u001b[32m[I 2022-11-24 13:30:53,715]\u001b[0m Trial 20 finished with value: 0.2969308157737391 and parameters: {'num_leaves': 51}. Best is trial 18 with value: 0.296868724263304.\u001b[0m\n",
      "num_leaves, val_score: 0.296869:  70%|#######   | 14/20 [1:49:16<51:37, 516.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1700]\tvalid_0's binary_logloss: 0.290111\tvalid_1's binary_logloss: 0.296931\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.377428 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.296869:  75%|#######5  | 15/20 [1:54:57<38:38, 463.66s/it]\u001b[32m[I 2022-11-24 13:36:35,329]\u001b[0m Trial 21 finished with value: 0.2971328138316498 and parameters: {'num_leaves': 128}. Best is trial 18 with value: 0.296868724263304.\u001b[0m\n",
      "num_leaves, val_score: 0.296869:  75%|#######5  | 15/20 [1:54:57<38:38, 463.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[585]\tvalid_0's binary_logloss: 0.290851\tvalid_1's binary_logloss: 0.297133\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.361484 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.293786\tvalid_1's binary_logloss: 0.297275\n",
      "[2000]\tvalid_0's binary_logloss: 0.290887\tvalid_1's binary_logloss: 0.296901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.296869:  80%|########  | 16/20 [2:07:35<36:48, 552.09s/it]\u001b[32m[I 2022-11-24 13:49:12,793]\u001b[0m Trial 22 finished with value: 0.2968797885563563 and parameters: {'num_leaves': 38}. Best is trial 18 with value: 0.296868724263304.\u001b[0m\n",
      "num_leaves, val_score: 0.296869:  80%|########  | 16/20 [2:07:35<36:48, 552.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2146]\tvalid_0's binary_logloss: 0.29052\tvalid_1's binary_logloss: 0.29688\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.323115 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.28934\tvalid_1's binary_logloss: 0.29695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.296869:  85%|########5 | 17/20 [2:16:09<27:01, 540.58s/it]\u001b[32m[I 2022-11-24 13:57:46,610]\u001b[0m Trial 23 finished with value: 0.29688725145367895 and parameters: {'num_leaves': 97}. Best is trial 18 with value: 0.296868724263304.\u001b[0m\n",
      "num_leaves, val_score: 0.296869:  85%|########5 | 17/20 [2:16:09<27:01, 540.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1123]\tvalid_0's binary_logloss: 0.28856\tvalid_1's binary_logloss: 0.296887\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.398782 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.296869:  90%|######### | 18/20 [2:21:42<15:56, 478.19s/it]\u001b[32m[I 2022-11-24 14:03:19,555]\u001b[0m Trial 24 finished with value: 0.2970431059249642 and parameters: {'num_leaves': 148}. Best is trial 18 with value: 0.296868724263304.\u001b[0m\n",
      "num_leaves, val_score: 0.296869:  90%|######### | 18/20 [2:21:42<15:56, 478.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[547]\tvalid_0's binary_logloss: 0.290506\tvalid_1's binary_logloss: 0.297043\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.343397 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.294983\tvalid_1's binary_logloss: 0.297719\n",
      "[2000]\tvalid_0's binary_logloss: 0.292587\tvalid_1's binary_logloss: 0.297019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.296869:  95%|#########5| 19/20 [2:34:11<09:19, 559.71s/it]\u001b[32m[I 2022-11-24 14:15:49,151]\u001b[0m Trial 25 finished with value: 0.29697891658332154 and parameters: {'num_leaves': 27}. Best is trial 18 with value: 0.296868724263304.\u001b[0m\n",
      "num_leaves, val_score: 0.296869:  95%|#########5| 19/20 [2:34:11<09:19, 559.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[2246]\tvalid_0's binary_logloss: 0.292128\tvalid_1's binary_logloss: 0.296979\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.343631 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.290852\tvalid_1's binary_logloss: 0.297102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_leaves, val_score: 0.296869: 100%|##########| 20/20 [2:42:15<00:00, 537.06s/it]\u001b[32m[I 2022-11-24 14:23:53,434]\u001b[0m Trial 26 finished with value: 0.29705057549198066 and parameters: {'num_leaves': 76}. Best is trial 18 with value: 0.296868724263304.\u001b[0m\n",
      "num_leaves, val_score: 0.296869: 100%|##########| 20/20 [2:42:15<00:00, 486.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1094]\tvalid_0's binary_logloss: 0.290338\tvalid_1's binary_logloss: 0.297051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.296869:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.350749 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.296869:  10%|#         | 1/10 [05:18<47:43, 318.12s/it]\u001b[32m[I 2022-11-24 14:29:11,573]\u001b[0m Trial 27 finished with value: 0.29719183625779927 and parameters: {'bagging_fraction': 0.6328762800926628, 'bagging_freq': 7}. Best is trial 27 with value: 0.29719183625779927.\u001b[0m\n",
      "bagging, val_score: 0.296869:  10%|#         | 1/10 [05:18<47:43, 318.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[658]\tvalid_0's binary_logloss: 0.293861\tvalid_1's binary_logloss: 0.297192\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.315420 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.296869:  20%|##        | 2/10 [11:13<45:18, 339.86s/it]\u001b[32m[I 2022-11-24 14:35:06,656]\u001b[0m Trial 28 finished with value: 0.2970914083780896 and parameters: {'bagging_fraction': 0.7208934779254887, 'bagging_freq': 2}. Best is trial 28 with value: 0.2970914083780896.\u001b[0m\n",
      "bagging, val_score: 0.296869:  20%|##        | 2/10 [11:13<45:18, 339.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[728]\tvalid_0's binary_logloss: 0.293414\tvalid_1's binary_logloss: 0.297091\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.340469 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.296869:  30%|###       | 3/10 [16:55<39:46, 340.88s/it]\u001b[32m[I 2022-11-24 14:40:48,739]\u001b[0m Trial 29 finished with value: 0.29715872804041726 and parameters: {'bagging_fraction': 0.9515265743436656, 'bagging_freq': 3}. Best is trial 28 with value: 0.2970914083780896.\u001b[0m\n",
      "bagging, val_score: 0.296869:  30%|###       | 3/10 [16:55<39:46, 340.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[699]\tvalid_0's binary_logloss: 0.293599\tvalid_1's binary_logloss: 0.297159\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.361776 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.296869:  40%|####      | 4/10 [22:50<34:40, 346.67s/it]\u001b[32m[I 2022-11-24 14:46:44,284]\u001b[0m Trial 30 finished with value: 0.2969744847778733 and parameters: {'bagging_fraction': 0.6647019368294569, 'bagging_freq': 3}. Best is trial 30 with value: 0.2969744847778733.\u001b[0m\n",
      "bagging, val_score: 0.296869:  40%|####      | 4/10 [22:50<34:40, 346.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[777]\tvalid_0's binary_logloss: 0.293146\tvalid_1's binary_logloss: 0.296974\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.368826 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.296869:  50%|#####     | 5/10 [28:59<29:33, 354.62s/it]\u001b[32m[I 2022-11-24 14:52:53,005]\u001b[0m Trial 31 finished with value: 0.2971066782560476 and parameters: {'bagging_fraction': 0.7804646624900535, 'bagging_freq': 7}. Best is trial 30 with value: 0.2969744847778733.\u001b[0m\n",
      "bagging, val_score: 0.296869:  50%|#####     | 5/10 [28:59<29:33, 354.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[798]\tvalid_0's binary_logloss: 0.293124\tvalid_1's binary_logloss: 0.297107\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.351395 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.296869:  60%|######    | 6/10 [35:01<23:48, 357.14s/it]\u001b[32m[I 2022-11-24 14:58:55,029]\u001b[0m Trial 32 finished with value: 0.29710273405529286 and parameters: {'bagging_fraction': 0.7412701333931924, 'bagging_freq': 7}. Best is trial 30 with value: 0.2969744847778733.\u001b[0m\n",
      "bagging, val_score: 0.296869:  60%|######    | 6/10 [35:01<23:48, 357.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[798]\tvalid_0's binary_logloss: 0.293137\tvalid_1's binary_logloss: 0.297103\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.435450 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.292095\tvalid_1's binary_logloss: 0.297118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.296869:  70%|#######   | 7/10 [42:29<19:19, 386.67s/it]\u001b[32m[I 2022-11-24 15:06:22,488]\u001b[0m Trial 33 finished with value: 0.29690919601654703 and parameters: {'bagging_fraction': 0.7462320749593332, 'bagging_freq': 4}. Best is trial 33 with value: 0.29690919601654703.\u001b[0m\n",
      "bagging, val_score: 0.296869:  70%|#######   | 7/10 [42:29<19:19, 386.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1032]\tvalid_0's binary_logloss: 0.291966\tvalid_1's binary_logloss: 0.296909\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.378484 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.292155\tvalid_1's binary_logloss: 0.296973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.296749:  80%|########  | 8/10 [52:28<15:08, 454.34s/it]\u001b[32m[I 2022-11-24 15:16:21,740]\u001b[0m Trial 34 finished with value: 0.2967494530700908 and parameters: {'bagging_fraction': 0.8695840030320283, 'bagging_freq': 2}. Best is trial 34 with value: 0.2967494530700908.\u001b[0m\n",
      "bagging, val_score: 0.296749:  80%|########  | 8/10 [52:28<15:08, 454.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1466]\tvalid_0's binary_logloss: 0.290227\tvalid_1's binary_logloss: 0.296749\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.319496 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.296749:  90%|######### | 9/10 [58:34<07:06, 426.69s/it]\u001b[32m[I 2022-11-24 15:22:27,634]\u001b[0m Trial 35 finished with value: 0.2971198635382194 and parameters: {'bagging_fraction': 0.4904159050205446, 'bagging_freq': 3}. Best is trial 34 with value: 0.2967494530700908.\u001b[0m\n",
      "bagging, val_score: 0.296749:  90%|######### | 9/10 [58:34<07:06, 426.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[774]\tvalid_0's binary_logloss: 0.293305\tvalid_1's binary_logloss: 0.29712\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.358038 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.29226\tvalid_1's binary_logloss: 0.296991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bagging, val_score: 0.296749: 100%|##########| 10/10 [1:05:51<00:00, 430.10s/it]\u001b[32m[I 2022-11-24 15:29:45,354]\u001b[0m Trial 36 finished with value: 0.2969820420298361 and parameters: {'bagging_fraction': 0.9878834045965024, 'bagging_freq': 5}. Best is trial 34 with value: 0.2967494530700908.\u001b[0m\n",
      "bagging, val_score: 0.296749: 100%|##########| 10/10 [1:05:51<00:00, 395.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[980]\tvalid_0's binary_logloss: 0.292337\tvalid_1's binary_logloss: 0.296982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.296749:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.335168 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.291997\tvalid_1's binary_logloss: 0.296908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.296749:  17%|#6        | 1/6 [08:44<43:42, 524.41s/it]\u001b[32m[I 2022-11-24 15:38:29,774]\u001b[0m Trial 37 finished with value: 0.29677534805965644 and parameters: {'feature_fraction': 0.9799999999999999}. Best is trial 37 with value: 0.29677534805965644.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.296749:  17%|#6        | 1/6 [08:44<43:42, 524.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1212]\tvalid_0's binary_logloss: 0.291061\tvalid_1's binary_logloss: 0.296775\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.365540 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.29214\tvalid_1's binary_logloss: 0.296975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.296734:  33%|###3      | 2/6 [18:50<38:10, 572.70s/it]\u001b[32m[I 2022-11-24 15:48:36,278]\u001b[0m Trial 38 finished with value: 0.2967336953198428 and parameters: {'feature_fraction': 0.852}. Best is trial 38 with value: 0.2967336953198428.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.296734:  33%|###3      | 2/6 [18:50<38:10, 572.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1466]\tvalid_0's binary_logloss: 0.290213\tvalid_1's binary_logloss: 0.296734\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.330047 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.292155\tvalid_1's binary_logloss: 0.296973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.296734:  50%|#####     | 3/6 [28:54<29:19, 586.66s/it]\u001b[32m[I 2022-11-24 15:58:39,548]\u001b[0m Trial 39 finished with value: 0.29674945307005035 and parameters: {'feature_fraction': 0.9159999999999999}. Best is trial 38 with value: 0.2967336953198428.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.296734:  50%|#####     | 3/6 [28:54<29:19, 586.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1466]\tvalid_0's binary_logloss: 0.290227\tvalid_1's binary_logloss: 0.296749\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.395617 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.29214\tvalid_1's binary_logloss: 0.296975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.296734:  67%|######6   | 4/6 [39:07<19:54, 597.29s/it]\u001b[32m[I 2022-11-24 16:08:53,134]\u001b[0m Trial 40 finished with value: 0.29673369531983107 and parameters: {'feature_fraction': 0.8839999999999999}. Best is trial 40 with value: 0.29673369531983107.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.296734:  67%|######6   | 4/6 [39:07<19:54, 597.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1466]\tvalid_0's binary_logloss: 0.290213\tvalid_1's binary_logloss: 0.296734\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.355762 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.292064\tvalid_1's binary_logloss: 0.296889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.296734:  83%|########3 | 5/6 [46:45<09:07, 547.05s/it]\u001b[32m[I 2022-11-24 16:16:31,095]\u001b[0m Trial 41 finished with value: 0.29685134164273824 and parameters: {'feature_fraction': 0.948}. Best is trial 40 with value: 0.29673369531983107.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.296734:  83%|########3 | 5/6 [46:45<09:07, 547.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1012]\tvalid_0's binary_logloss: 0.292006\tvalid_1's binary_logloss: 0.296851\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.327842 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.292199\tvalid_1's binary_logloss: 0.296985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature_fraction_stage2, val_score: 0.296734: 100%|##########| 6/6 [55:15<00:00, 534.50s/it]\u001b[32m[I 2022-11-24 16:25:01,240]\u001b[0m Trial 42 finished with value: 0.29685651851389844 and parameters: {'feature_fraction': 0.82}. Best is trial 40 with value: 0.29673369531983107.\u001b[0m\n",
      "feature_fraction_stage2, val_score: 0.296734: 100%|##########| 6/6 [55:15<00:00, 552.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1172]\tvalid_0's binary_logloss: 0.291456\tvalid_1's binary_logloss: 0.296857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.296734:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.334629 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.293255\tvalid_1's binary_logloss: 0.296668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.296380:   5%|5         | 1/20 [10:33<3:20:43, 633.85s/it]\u001b[32m[I 2022-11-24 16:35:35,107]\u001b[0m Trial 43 finished with value: 0.29637977797543985 and parameters: {'lambda_l1': 0.026145887590972736, 'lambda_l2': 9.687049323509656}. Best is trial 43 with value: 0.29637977797543985.\u001b[0m\n",
      "regularization_factors, val_score: 0.296380:   5%|5         | 1/20 [10:33<3:20:43, 633.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1466]\tvalid_0's binary_logloss: 0.291811\tvalid_1's binary_logloss: 0.29638\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.346390 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.292343\tvalid_1's binary_logloss: 0.296872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.296380:  10%|#         | 2/20 [18:22<2:41:00, 536.70s/it]\u001b[32m[I 2022-11-24 16:43:23,797]\u001b[0m Trial 44 finished with value: 0.29681216383062387 and parameters: {'lambda_l1': 2.684560090201724e-06, 'lambda_l2': 0.3589806774852525}. Best is trial 43 with value: 0.29637977797543985.\u001b[0m\n",
      "regularization_factors, val_score: 0.296380:  10%|#         | 2/20 [18:22<2:41:00, 536.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1012]\tvalid_0's binary_logloss: 0.292288\tvalid_1's binary_logloss: 0.296812\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.396035 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.29215\tvalid_1's binary_logloss: 0.296986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.296380:  15%|#5        | 3/20 [26:05<2:22:28, 502.85s/it]\u001b[32m[I 2022-11-24 16:51:06,361]\u001b[0m Trial 45 finished with value: 0.29692601050821504 and parameters: {'lambda_l1': 2.5955618682116844e-05, 'lambda_l2': 1.6420952397328504e-07}. Best is trial 43 with value: 0.29637977797543985.\u001b[0m\n",
      "regularization_factors, val_score: 0.296380:  15%|#5        | 3/20 [26:05<2:22:28, 502.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1012]\tvalid_0's binary_logloss: 0.29209\tvalid_1's binary_logloss: 0.296926\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.351863 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.292165\tvalid_1's binary_logloss: 0.296944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.296380:  20%|##        | 4/20 [36:11<2:24:57, 543.56s/it]\u001b[32m[I 2022-11-24 17:01:12,345]\u001b[0m Trial 46 finished with value: 0.29669279176897567 and parameters: {'lambda_l1': 2.71858806399802e-08, 'lambda_l2': 0.011891848127010494}. Best is trial 43 with value: 0.29637977797543985.\u001b[0m\n",
      "regularization_factors, val_score: 0.296380:  20%|##        | 4/20 [36:11<2:24:57, 543.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1466]\tvalid_0's binary_logloss: 0.290223\tvalid_1's binary_logloss: 0.296693\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.393295 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.292281\tvalid_1's binary_logloss: 0.296765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.296380:  25%|##5       | 5/20 [44:43<2:13:05, 532.35s/it]\u001b[32m[I 2022-11-24 17:09:44,799]\u001b[0m Trial 47 finished with value: 0.2966476179012632 and parameters: {'lambda_l1': 2.038392448529123e-08, 'lambda_l2': 0.23215518002580493}. Best is trial 43 with value: 0.29637977797543985.\u001b[0m\n",
      "regularization_factors, val_score: 0.296380:  25%|##5       | 5/20 [44:43<2:13:05, 532.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1172]\tvalid_0's binary_logloss: 0.291556\tvalid_1's binary_logloss: 0.296648\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.358926 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.29217\tvalid_1's binary_logloss: 0.296905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.296380:  30%|###       | 6/20 [54:49<2:10:05, 557.54s/it]\u001b[32m[I 2022-11-24 17:19:51,247]\u001b[0m Trial 48 finished with value: 0.2966734434338358 and parameters: {'lambda_l1': 0.0001367880914817886, 'lambda_l2': 0.039771556976660793}. Best is trial 43 with value: 0.29637977797543985.\u001b[0m\n",
      "regularization_factors, val_score: 0.296380:  30%|###       | 6/20 [54:50<2:10:05, 557.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1465]\tvalid_0's binary_logloss: 0.290259\tvalid_1's binary_logloss: 0.296673\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.385951 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.292895\tvalid_1's binary_logloss: 0.296629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.296380:  35%|###5      | 7/20 [1:03:36<1:58:36, 547.45s/it]\u001b[32m[I 2022-11-24 17:28:37,934]\u001b[0m Trial 49 finished with value: 0.2965047417626081 and parameters: {'lambda_l1': 0.00023261636830484742, 'lambda_l2': 3.7818985655916664}. Best is trial 43 with value: 0.29637977797543985.\u001b[0m\n",
      "regularization_factors, val_score: 0.296380:  35%|###5      | 7/20 [1:03:36<1:58:36, 547.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1172]\tvalid_0's binary_logloss: 0.292271\tvalid_1's binary_logloss: 0.296505\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.341930 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.292144\tvalid_1's binary_logloss: 0.296994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.296380:  40%|####      | 8/20 [1:12:07<1:47:09, 535.77s/it]\u001b[32m[I 2022-11-24 17:37:08,678]\u001b[0m Trial 50 finished with value: 0.2968858865037413 and parameters: {'lambda_l1': 8.6487754418767e-06, 'lambda_l2': 3.808588006834954e-05}. Best is trial 43 with value: 0.29637977797543985.\u001b[0m\n",
      "regularization_factors, val_score: 0.296380:  40%|####      | 8/20 [1:12:07<1:47:09, 535.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1172]\tvalid_0's binary_logloss: 0.291398\tvalid_1's binary_logloss: 0.296886\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.384159 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.29214\tvalid_1's binary_logloss: 0.296975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.296380:  45%|####5     | 9/20 [1:20:39<1:36:50, 528.21s/it]\u001b[32m[I 2022-11-24 17:45:40,278]\u001b[0m Trial 51 finished with value: 0.2968717929134868 and parameters: {'lambda_l1': 6.861466002703938e-07, 'lambda_l2': 8.937920731372487e-07}. Best is trial 43 with value: 0.29637977797543985.\u001b[0m\n",
      "regularization_factors, val_score: 0.296380:  45%|####5     | 9/20 [1:20:39<1:36:50, 528.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1172]\tvalid_0's binary_logloss: 0.291391\tvalid_1's binary_logloss: 0.296872\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.359129 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.292138\tvalid_1's binary_logloss: 0.296997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.296380:  50%|#####     | 10/20 [1:28:22<1:24:41, 508.12s/it]\u001b[32m[I 2022-11-24 17:53:23,414]\u001b[0m Trial 52 finished with value: 0.29692561189855 and parameters: {'lambda_l1': 5.651035226127556e-05, 'lambda_l2': 2.1868644760027366e-05}. Best is trial 43 with value: 0.29637977797543985.\u001b[0m\n",
      "regularization_factors, val_score: 0.296380:  50%|#####     | 10/20 [1:28:22<1:24:41, 508.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1012]\tvalid_0's binary_logloss: 0.29207\tvalid_1's binary_logloss: 0.296926\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.344134 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.292192\tvalid_1's binary_logloss: 0.296936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.296380:  55%|#####5    | 11/20 [1:38:32<1:20:53, 539.32s/it]\u001b[32m[I 2022-11-24 18:03:33,474]\u001b[0m Trial 53 finished with value: 0.296664554798021 and parameters: {'lambda_l1': 0.2639802097828259, 'lambda_l2': 0.0010499901339967458}. Best is trial 43 with value: 0.29637977797543985.\u001b[0m\n",
      "regularization_factors, val_score: 0.296380:  55%|#####5    | 11/20 [1:38:32<1:20:53, 539.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1466]\tvalid_0's binary_logloss: 0.290246\tvalid_1's binary_logloss: 0.296665\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.347327 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.293234\tvalid_1's binary_logloss: 0.296714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.296380:  60%|######    | 12/20 [1:46:36<1:09:41, 522.64s/it]\u001b[32m[I 2022-11-24 18:11:37,974]\u001b[0m Trial 54 finished with value: 0.2966612120502167 and parameters: {'lambda_l1': 0.03151904007160143, 'lambda_l2': 9.03838147841089}. Best is trial 43 with value: 0.29637977797543985.\u001b[0m\n",
      "regularization_factors, val_score: 0.296380:  60%|######    | 12/20 [1:46:36<1:09:41, 522.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1012]\tvalid_0's binary_logloss: 0.293194\tvalid_1's binary_logloss: 0.296661\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.334991 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.293217\tvalid_1's binary_logloss: 0.296643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.296332:  65%|######5   | 13/20 [1:58:05<1:06:51, 573.06s/it]\u001b[32m[I 2022-11-24 18:23:07,042]\u001b[0m Trial 55 finished with value: 0.29633249059881045 and parameters: {'lambda_l1': 0.0035714307987461313, 'lambda_l2': 8.837218918956209}. Best is trial 55 with value: 0.29633249059881045.\u001b[0m\n",
      "regularization_factors, val_score: 0.296332:  65%|######5   | 13/20 [1:58:05<1:06:51, 573.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1466]\tvalid_0's binary_logloss: 0.291741\tvalid_1's binary_logloss: 0.296332\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.346412 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.293146\tvalid_1's binary_logloss: 0.29667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.296332:  70%|#######   | 14/20 [2:08:48<59:23, 593.99s/it]  \u001b[32m[I 2022-11-24 18:33:49,403]\u001b[0m Trial 56 finished with value: 0.29634880732809454 and parameters: {'lambda_l1': 0.018555258715679104, 'lambda_l2': 7.531478994683672}. Best is trial 55 with value: 0.29633249059881045.\u001b[0m\n",
      "regularization_factors, val_score: 0.296332:  70%|#######   | 14/20 [2:08:48<59:23, 593.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1466]\tvalid_0's binary_logloss: 0.291632\tvalid_1's binary_logloss: 0.296349\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.320779 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.292162\tvalid_1's binary_logloss: 0.297006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.296332:  75%|#######5  | 15/20 [2:19:02<50:00, 600.09s/it]\u001b[32m[I 2022-11-24 18:44:03,639]\u001b[0m Trial 57 finished with value: 0.2967548961999626 and parameters: {'lambda_l1': 0.005390818286891427, 'lambda_l2': 0.004266690999450565}. Best is trial 55 with value: 0.29633249059881045.\u001b[0m\n",
      "regularization_factors, val_score: 0.296332:  75%|#######5  | 15/20 [2:19:02<50:00, 600.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1466]\tvalid_0's binary_logloss: 0.290202\tvalid_1's binary_logloss: 0.296755\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.397488 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.292783\tvalid_1's binary_logloss: 0.296638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.296332:  80%|########  | 16/20 [2:28:02<38:47, 581.97s/it]\u001b[32m[I 2022-11-24 18:53:03,531]\u001b[0m Trial 58 finished with value: 0.2965169460401345 and parameters: {'lambda_l1': 2.3615801208910714, 'lambda_l2': 0.28954239273301674}. Best is trial 55 with value: 0.29633249059881045.\u001b[0m\n",
      "regularization_factors, val_score: 0.296332:  80%|########  | 16/20 [2:28:02<38:47, 581.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1172]\tvalid_0's binary_logloss: 0.292139\tvalid_1's binary_logloss: 0.296517\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.354983 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.292547\tvalid_1's binary_logloss: 0.296752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.296332:  85%|########5 | 17/20 [2:36:39<28:07, 562.62s/it]\u001b[32m[I 2022-11-24 19:01:41,159]\u001b[0m Trial 59 finished with value: 0.296622674821875 and parameters: {'lambda_l1': 0.0011737250504980986, 'lambda_l2': 1.0391610941150404}. Best is trial 55 with value: 0.29633249059881045.\u001b[0m\n",
      "regularization_factors, val_score: 0.296332:  85%|########5 | 17/20 [2:36:39<28:07, 562.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1172]\tvalid_0's binary_logloss: 0.291848\tvalid_1's binary_logloss: 0.296623\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.335604 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.292999\tvalid_1's binary_logloss: 0.296567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.296332:  90%|######### | 18/20 [2:45:33<18:28, 554.03s/it]\u001b[32m[I 2022-11-24 19:10:35,178]\u001b[0m Trial 60 finished with value: 0.2964568429944993 and parameters: {'lambda_l1': 3.7211423598494906, 'lambda_l2': 0.00010507366937590468}. Best is trial 55 with value: 0.29633249059881045.\u001b[0m\n",
      "regularization_factors, val_score: 0.296332:  90%|######### | 18/20 [2:45:33<18:28, 554.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1172]\tvalid_0's binary_logloss: 0.292407\tvalid_1's binary_logloss: 0.296457\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.334739 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.292223\tvalid_1's binary_logloss: 0.296791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.296332:  95%|#########5| 19/20 [2:53:23<08:48, 528.66s/it]\u001b[32m[I 2022-11-24 19:18:24,725]\u001b[0m Trial 61 finished with value: 0.29671453179732654 and parameters: {'lambda_l1': 0.37587257398481166, 'lambda_l2': 0.04520677660352194}. Best is trial 55 with value: 0.29633249059881045.\u001b[0m\n",
      "regularization_factors, val_score: 0.296332:  95%|#########5| 19/20 [2:53:23<08:48, 528.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1012]\tvalid_0's binary_logloss: 0.292167\tvalid_1's binary_logloss: 0.296715\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.365047 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.292142\tvalid_1's binary_logloss: 0.296996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "regularization_factors, val_score: 0.296332: 100%|##########| 20/20 [3:01:09<00:00, 509.93s/it]\u001b[32m[I 2022-11-24 19:26:11,017]\u001b[0m Trial 62 finished with value: 0.2969353710125944 and parameters: {'lambda_l1': 0.0030565786889789693, 'lambda_l2': 2.6267293658915582e-08}. Best is trial 55 with value: 0.29633249059881045.\u001b[0m\n",
      "regularization_factors, val_score: 0.296332: 100%|##########| 20/20 [3:01:09<00:00, 543.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1012]\tvalid_0's binary_logloss: 0.292083\tvalid_1's binary_logloss: 0.296935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.296332:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.352943 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.293194\tvalid_1's binary_logloss: 0.296635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.296332:  20%|##        | 1/5 [09:04<36:17, 544.39s/it]\u001b[32m[I 2022-11-24 19:35:15,423]\u001b[0m Trial 63 finished with value: 0.2965083981366624 and parameters: {'min_child_samples': 100}. Best is trial 63 with value: 0.2965083981366624.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.296332:  20%|##        | 1/5 [09:04<36:17, 544.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1172]\tvalid_0's binary_logloss: 0.292621\tvalid_1's binary_logloss: 0.296508\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.328828 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.293241\tvalid_1's binary_logloss: 0.296709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.296332:  40%|####      | 2/5 [18:18<27:30, 550.18s/it]\u001b[32m[I 2022-11-24 19:44:29,656]\u001b[0m Trial 64 finished with value: 0.2965387588542164 and parameters: {'min_child_samples': 5}. Best is trial 63 with value: 0.2965083981366624.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.296332:  40%|####      | 2/5 [18:18<27:30, 550.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1218]\tvalid_0's binary_logloss: 0.292513\tvalid_1's binary_logloss: 0.296539\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.359530 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.293244\tvalid_1's binary_logloss: 0.296693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.296332:  60%|######    | 3/5 [28:51<19:35, 587.98s/it]\u001b[32m[I 2022-11-24 19:55:02,614]\u001b[0m Trial 65 finished with value: 0.29635957964665455 and parameters: {'min_child_samples': 25}. Best is trial 65 with value: 0.29635957964665455.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.296332:  60%|######    | 3/5 [28:51<19:35, 587.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1466]\tvalid_0's binary_logloss: 0.29173\tvalid_1's binary_logloss: 0.29636\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.357666 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.293214\tvalid_1's binary_logloss: 0.296699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.296332:  80%|########  | 4/5 [36:57<09:07, 547.83s/it]\u001b[32m[I 2022-11-24 20:03:08,891]\u001b[0m Trial 66 finished with value: 0.2966400554953652 and parameters: {'min_child_samples': 50}. Best is trial 65 with value: 0.29635957964665455.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.296332:  80%|########  | 4/5 [36:57<09:07, 547.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1012]\tvalid_0's binary_logloss: 0.293174\tvalid_1's binary_logloss: 0.29664\n",
      "[LightGBM] [Info] Number of positive: 3312365, number of negative: 3312365\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.383575 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1010\n",
      "[LightGBM] [Info] Number of data points in the train set: 6624730, number of used features: 23\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[1000]\tvalid_0's binary_logloss: 0.29322\tvalid_1's binary_logloss: 0.296654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "min_data_in_leaf, val_score: 0.296332: 100%|##########| 5/5 [47:31<00:00, 578.68s/it]\u001b[32m[I 2022-11-24 20:13:42,265]\u001b[0m Trial 67 finished with value: 0.29634651726734945 and parameters: {'min_child_samples': 10}. Best is trial 67 with value: 0.29634651726734945.\u001b[0m\n",
      "min_data_in_leaf, val_score: 0.296332: 100%|##########| 5/5 [47:31<00:00, 570.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[1466]\tvalid_0's binary_logloss: 0.291759\tvalid_1's binary_logloss: 0.296347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 学習\n",
    "vl_pred_df = valid[[\"session\", \"aid\", \"type\"]].copy()\n",
    "\n",
    "tr_x, tr_y = train[features], train[target]\n",
    "vl_x, vl_y = valid[features], valid[target]\n",
    "tr_data = lgb.Dataset(tr_x, label=tr_y)\n",
    "vl_data = lgb.Dataset(vl_x, label=vl_y)\n",
    "\n",
    "model = optuna_lgb.train(params, tr_data, valid_sets=[tr_data, vl_data],\n",
    "                         num_boost_round=2000000, early_stopping_rounds=100, verbose_eval=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'objective': 'binary', 'boosting': 'gbdt', 'learning_rate': 0.1, 'metric': 'binary_logloss', 'seed': 42, 'feature_pre_filter': False, 'lambda_l1': 0.0035714307987461313, 'lambda_l2': 8.837218918956209, 'num_leaves': 56, 'feature_fraction': 0.8839999999999999, 'bagging_fraction': 0.8695840030320283, 'bagging_freq': 2, 'min_child_samples': 20, 'num_iterations': 2000000, 'early_stopping_round': 100}\n"
     ]
    }
   ],
   "source": [
    "print(model.params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
